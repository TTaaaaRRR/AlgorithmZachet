## Оглавление (тыкается)
* [Билет 1](#bilet1)
* [Билет 2](#bilet2)
* [Билет 3](#bilet3)
* [Билет 4](#bilet4)
* [Билет 5](#bilet5)
* [Билет 6](#bilet6)
* [Билет 7](#bilet7)
* [Билет 8](#bilet8)
* [Билет 9](#bilet9)
* [Билет 10](#bilet10)
* [Билет 11](#bilet11)

<a name="bilet1">.</a> 
## Билет 1
Базовое понятие алгоритма. Основные определения, примеры. Свойства алгоритмов.  Модель вычислений. RAM-модель. Элементарные (встроенные) типы данных, операции над ними. Понятие сложности алгоритмов. Понятия О-большое, Омега-большое. Описательная, пространственная, вычислительная сложности вложенных циклов. 

### 1. Понятие и свойства алгоритма

**Алгоритм** — это конечная последовательность чётко определённых действий (инструкций) для решения конкретной задачи.

* **Пример из жизни:** Рецепт торта.
* **Простой пример на Python:** Алгоритм поиска максимального элемента в списке.

```python
def find_max(data_list):
  max_value = data_list[0]
  for item in data_list:
    if item > max_value:
      max_value = item
  return max_value
```

***

### 2. Свойства алгоритмов

* **Дискретность:** Алгоритм состоит из отдельных шагов.
* **Детерминированность (Определённость):** Каждый шаг строго определён и не допускает двусмысленности.
* **Конечность:** Алгоритм должен завершиться за конечное число шагов.
* **Массовость:** Алгоритм применим к разным наборам исходных данных.
* **Результативность:** Алгоритм должен приводить к правильному результату.

***

### 3. Модель вычислений (RAM-модель)

**Модель вычислений** — это абстрактная машина, определяющая базовый набор операций.

**RAM-модель (Random Access Machine)** — наиболее распространённая модель:
* Память состоит из ячеек с произвольным доступом (чтение/запись за $O(1)$).
* Все базовые арифметические и логические операции (+, -, *, /, and, or, if) выполняются за константное время — $O(1)$.
* Инструкции выполняются последовательно.

***

### 4. Элементарные типы данных

Это встроенные в язык типы данных. В Python это:
* **int:** Целые числа (`5`, `-10`). Операции: `+`, `-`, `*`, `/`, `%`.
* **float:** Числа с плавающей точкой (`3.14`, `-0.5`). Операции: `+`, `-`, `*`, `/`.
* **bool:** Логический тип (`True`, `False`). Операции: `and`, `or`, `not`.
* **str:** Строки (`"hello"`). Операции: `+` (конкатенация).

***

### 5. Понятие сложности алгоритмов

**Сложность алгоритма** — это мера ресурсов (времени или памяти), которые требуются алгоритму для выполнения, как функция от размера входных данных `n`.

* **Временная сложность (Time Complexity):** Сколько времени работает алгоритм.
* **Пространственная сложность (Space Complexity):** Сколько дополнительной памяти использует алгоритм.

***

### 6. O-большое и $\Omega$-большое

Это асимптотические нотации для оценки сложности.

* **O-большое (Big O):** Верхняя граница, **худший случай**. Описывает, как медленно может работать алгоритм. Например, $O(n^2)$ означает, что время работы растёт не быстрее, чем квадрат от размера входных данных.

* **$\Omega$-большое (Big Omega):** Нижняя граница, **лучший случай**. Описывает, как быстро может работать алгоритм. Например, $\Omega(n)$ означает, что в лучшем случае алгоритм выполнится не быстрее, чем за линейное время.

***

### 7. Сложности вложенных циклов
<a name="твоё_название"></a> 
Рассмотрим пример:

```python
def process_matrix(matrix):
  n = len(matrix)
  for i in range(n):      # Внешний цикл
    for j in range(n):    # Внутренний цикл
      print(matrix[i][j]) # Операция за O(1)
```

* **Описательная сложность:** Время работы алгоритма пропорционально квадрату размера входной матрицы.
* **Пространственная сложность:** Алгоритм использует несколько переменных (`n`, `i`, `j`), их количество не зависит от размера `matrix`. Сложность по памяти — $O(1)$ (константная).
* **Вычислительная (временная) сложность:** Внешний цикл выполняется `n` раз, и для каждой итерации внутренний цикл также выполняется `n` раз. Общее число операций: $n \times n = n^2$. Сложность — $O(n^2)$.


<a name="bilet2">.</a> 
## Билет 2.
Линейные типы данных. Массив, односвязный список, двусвязный список. Интерфейс списка. Операции добавления, удаления, поиска элемента по значению, определение максимума массива. Квадратичные методы сортировки. Сортировки за O(n*log n).

### 1. Линейные структуры данных

Это структуры, в которых элементы расположены **последовательно**, один за другим.

***

### 2. Массив (Array)

**Массив** — это структура данных, хранящая элементы одного типа в **непрерывном блоке памяти**. Главная особенность — **быстрый доступ к элементу по индексу за $O(1)$**.

* **Плюсы:** Быстрый доступ по индексу.
* **Минусы:** Медленное добавление/удаление элементов в середину ($O(n)$), так как требует сдвига других элементов.
* **В Python:** роль массива выполняет встроенный тип `list` (динамический массив).

```python
# Массив в Python — это list
my_array = [10, 20, 30, 40]
print(my_array[1]) # -> 20 (доступ за O(1))
```

***

### 3. Односвязный список (Singly Linked List)

**Односвязный список** — это набор **узлов (nodes)**, где каждый узел содержит данные и **ссылку (указатель) на следующий узел**. Доступ к элементам последовательный, начиная с головы (`head`).

* **Плюсы:** Быстрое добавление/удаление в начало списка ($O(1)$).
* **Минусы:** Медленный доступ к элементу по индексу и поиск ($O(n)$).

```python
# Упрощенная структура узла
class Node:
  def __init__(self, data=None):
    self.data = data
    self.next = None
```

***

### 4. Двусвязный список (Doubly Linked List)

Похож на односвязный, но каждый узел также имеет **ссылку на предыдущий элемент**.

* **Плюсы:** Быстрое добавление/удаление в начало и конец ($O(1)$). Можно проходить по списку в обе стороны.
* **Минусы:** Занимает немного больше памяти из-за второй ссылки.

```python
# Упрощенная структура узла
class Node:
  def __init__(self, data=None):
    self.data = data
    self.next = None
    self.prev = None
```

***

### 5. Сравнение и интерфейс списка

| Операция | Массив (list) | Односвязный список | Двусвязный список |
| :--- | :---: | :---: | :---: |
| **Доступ по индексу** | $O(1)$ | $O(n)$ | $O(n)$ |
| **Поиск по значению** | $O(n)$ | $O(n)$ | $O(n)$ |
| **Вставка/удаление в начале** | $O(n)$ | $O(1)$ | $O(1)$ |
| **Вставка/удаление в конце**| $O(1)$ (в среднем) | $O(n)$ или $O(1)$* | $O(1)$ |
| **Вставка/удаление в середине**| $O(n)$ | $O(n)$ | $O(n)$ |
*\*Если хранить ссылку на хвост (tail), то $O(1)$.*

***

### 6. Основные операции для массива (Python `list`)

```python
data = [10, 50, 20, 90, 40]

# 1. Добавление (в конец)
data.append(60) # O(1)
print(data) # -> [10, 50, 20, 90, 40, 60]

# 2. Удаление (по значению)
data.remove(20) # O(n)
print(data) # -> [10, 50, 90, 40, 60]

# 3. Поиск элемента по значению (возвращает индекс)
try:
    index = data.index(90) # O(n)
    print(f"Элемент 90 на позиции: {index}") # -> Элемент 90 на позиции: 2
except ValueError:
    print("Элемент не найден")

# 4. Определение максимума
max_value = max(data) # O(n)
print(f"Максимум: {max_value}") # -> Максимум: 90
```

***

### 7. Квадратичные сортировки ($O(n^2)$) 🐢

Эти алгоритмы просты в реализации, но неэффективны на больших данных. В основе лежат вложенные циклы.

* **Сортировка пузырьком (Bubble Sort):** Соседние элементы сравниваются и меняются местами, если они в неправильном порядке.
* **Сортировка выбором (Selection Sort):** Находим минимальный элемент и ставим его в начало, затем ищем следующий минимальный для оставшейся части и т.д.
* **Сортировка вставками (Insertion Sort):** Берём элементы по одному и "вставляем" их на правильное место в уже отсортированной части массива.

**Пример: Сортировка пузырьком**
```python
def bubble_sort(arr):
  n = len(arr)
  for i in range(n):
    for j in range(0, n-i-1):
      if arr[j] > arr[j+1]:
        arr[j], arr[j+1] = arr[j+1], arr[j] # Обмен
  return arr

print(bubble_sort([64, 34, 25, 12, 22, 11, 90]))
```

***

### 8. Быстрые сортировки ($O(n \log n)$) 🚀

Гораздо эффективнее. Используют принцип **"разделяй и властвуй"**.

* **Сортировка слиянием (Merge Sort):** Массив рекурсивно делится пополам до тех пор, пока не останутся единичные элементы. Затем отсортированные части "сливаются" вместе.
* **Быстрая сортировка (Quick Sort):** Выбирается "опорный" элемент (pivot). Массив делится на две части: элементы меньше опорного и элементы больше опорного. Затем части рекурсивно сортируются.

**Пример: Быстрая сортировка**
```python
def quick_sort(arr):
  if len(arr) <= 1:
    return arr
  else:
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)

print(quick_sort([64, 34, 25, 12, 22, 11, 90]))
```

<a name="bilet3">.</a> 
## Билет 3. 
Рекурсивные методы в программировании. Основные понятия, рекурсивные функции и процедуры. Декомпозиционный подход к синтезу алгоритмов на примере задачи о Ханойских башнях.

### 1. Основные понятия рекурсии

**Рекурсия** — это метод в программировании, при котором функция вызывает саму себя для решения задачи.

Ключевые компоненты любой рекурсивной функции:
* **Базовый случай (условие выхода):** Простое условие, при котором функция прекращает вызывать себя и возвращает конкретный результат. Это предотвращает бесконечный цикл.
* **Рекурсивный шаг (шаг рекурсии):** Действие, которое сводит исходную задачу к более простой подзадаче того же типа и вызывает функцию для её решения.

Аналогия из жизни — **матрёшка**. Чтобы открыть всю матрёшку (задача), нужно открыть текущую куклу (рекурсивный шаг) и обнаружить внутри неё матрёшку поменьше (подзадача). Так продолжается до тех пор, пока мы не дойдем до самой маленькой, неразборной куклы (базовый случай).

***

### 2. Рекурсивные функции и процедуры

* **Рекурсивная функция** — это функция, которая вызывает саму себя и **возвращает значение**.
* **Рекурсивная процедура** — это процедура (в Python тоже `def`), которая вызывает саму себя, но **не возвращает значение**, а выполняет какое-либо действие (например, печать на экран).

**Классический пример рекурсивной функции — вычисление факториала $n!$.**

* **Задача:** Вычислить $n! = n \cdot (n-1) \cdot \dots \cdot 1$.
* **Базовый случай:** Если $n=0$ или $n=1$, то факториал равен 1.
* **Рекурсивный шаг:** Для любого другого $n$, факториал равен $n \cdot (n-1)!$.

```python
def factorial(n):
  # Базовый случай
  if n <= 1:
    return 1
  # Рекурсивный шаг
  else:
    return n * factorial(n - 1)

print(f"5! = {factorial(5)}") # -> 5! = 120
```

***

### 3. Декомпозиция на примере Ханойских башен

**Декомпозиционный подход** — это метод решения сложной задачи путём её разбиения на более мелкие и простые **подзадачи того же типа**. Рекурсия идеально подходит для реализации этого подхода.

**Задача о Ханойских башнях:**
Переместить стопку из `n` дисков с одного стержня (`A`) на другой (`C`), используя третий вспомогательный стержень (`B`).

**Правила:**
1.  За раз можно перемещать только один диск.
2.  Нельзя класть больший диск на меньший.

**Декомпозиция и рекурсивный синтез:**
Чтобы переместить `n` дисков со стержня `A` на `C`, нужно:

1.  **Решить подзадачу:** Переместить стопку поменьше из `n-1` дисков со стержня `A` на вспомогательный `B`.
2.  **Выполнить базовое действие:** Переместить самый большой, `n`-ый диск со стержня `A` на целевой `C`.
3.  **Решить подзадачу:** Переместить стопку из `n-1` дисков со вспомогательного стержня `B` на целевой `C`.

Этот алгоритм элегантно реализуется через рекурсивную процедуру.

```python
def hanoi(n, source, destination, auxiliary):
  """
  Рекурсивная процедура для решения задачи о Ханойских башнях.
  Печатает последовательность ходов.
  """
  # Базовый случай: если нужно переместить 0 дисков, ничего не делаем
  if n > 0:
    # 1. Перемещаем n-1 дисков с ИСТОЧНИКА на ВСПОМОГАТЕЛЬНЫЙ
    hanoi(n - 1, source, auxiliary, destination)
    
    # 2. Перемещаем оставшийся самый большой диск с ИСТОЧНИКА на ЦЕЛЕВОЙ
    print(f"Переместить диск {n} с {source} на {destination}")
    
    # 3. Перемещаем n-1 дисков со ВСПОМОГАТЕЛЬНОГО на ЦЕЛЕВОЙ
    hanoi(n - 1, auxiliary, destination, source)

# Пример вызова для 3 дисков
print("Последовательность ходов для 3 дисков:")
hanoi(3, 'A', 'C', 'B')
```

<a name="bilet4">.</a> 
## Билет 4. 
Численное решение уравнений. Бинарный поиск. Эквивалентность рекурсии и итерации на примере вычисления факториала.

Конечно, вот разбор четвёртого билета.

### 1. Численное решение уравнений: Метод деления пополам

**Метод деления пополам (бисекции)** — это численный алгоритм для нахождения корня уравнения $f(x) = 0$. Он работает на **непрерывной функции** и по своей сути является **бинарным поиском** для поиска корня.

**Идея:**
1.  Берётся отрезок $[a, b]$, на концах которого функция $f(x)$ имеет **разные знаки** (т.е., $f(a) \cdot f(b) < 0$). Это гарантирует, что внутри отрезка есть хотя бы один корень.
2.  Находится середина отрезка $c = (a + b) / 2$.
3.  Вычисляется значение $f(c)$.
4.  Если $f(c)$ имеет тот же знак, что и $f(a)$, то корень находится на отрезке $[c, b]$. В противном случае — на отрезке $[a, c]$.
5.  Процесс повторяется, сужая отрезок вдвое на каждой итерации, пока не будет достигнута нужная точность.

**Сложность:** Количество итераций для достижения точности $\epsilon$ равно $O(\log_2((b-a)/\epsilon))$.

```python
def bisection_method(f, a, b, tolerance=1e-7):
  """Находит корень функции f на отрезке [a, b] методом деления пополам."""
  if f(a) * f(b) >= 0:
    return "На отрезке нет корня или их четное количество."
  
  while (b - a) / 2.0 > tolerance:
    midpoint = (a + b) / 2.0
    if f(midpoint) == 0:
      return midpoint # Корень найден точно
    elif f(a) * f(midpoint) < 0:
      b = midpoint # Корень в левой половине
    else:
      a = midpoint # Корень в правой половине
      
  return (a + b) / 2.0

# Найдем корень уравнения x^2 - 4 = 0 на отрезке [0, 5]
def my_function(x):
  return x**2 - 4

root = bisection_method(my_function, 0, 5)
print(f"Корень уравнения x^2 - 4 = 0 примерно равен: {root:.7f}")
# -> Корень уравнения x^2 - 4 = 0 примерно равен: 2.0000000
```

---

### 2. Бинарный поиск

**Бинарный поиск** — это высокоэффективный алгоритм поиска элемента в **отсортированном массиве**. 🕵️‍♂️

**Алгоритм:**
1.  Определяем левую (`left`) и правую (`right`) границы поиска.
2.  Находим средний элемент `mid = (left + right) // 2`.
3.  Сравниваем искомый элемент с `arr[mid]`:
    * Если они равны — элемент найден.
    * Если искомый элемент **меньше** — сдвигаем правую границу `right = mid - 1` и ищем в левой половине.
    * Если искомый элемент **больше** — сдвигаем левую границу `left = mid + 1` и ищем в правой половине.
4.  Шаги 2-3 повторяются, пока `left <= right`. Если цикл завершился, а элемент не найден, значит, его нет в массиве.

**Сложность:** Временная сложность — $O(\log n)$, пространственная — $O(1)$.

```python
def binary_search(arr, target):
  """Ищет target в отсортированном массиве arr и возвращает его индекс."""
  left, right = 0, len(arr) - 1
  
  while left <= right:
    mid = (left + right) // 2
    if arr[mid] == target:
      return mid # Элемент найден
    elif arr[mid] < target:
      left = mid + 1 # Искать в правой половине
    else:
      right = mid - 1 # Искать в левой половине
      
  return -1 # Элемент не найден

sorted_array = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]
index = binary_search(sorted_array, 23)
print(f"Элемент 23 находится по индексу: {index}") # -> 5
```

---

### 3. Эквивалентность рекурсии и итерации

Любой алгоритм, реализованный с помощью **рекурсии**, можно реализовать с помощью **итерации (циклов)**, и наоборот. Выбор между ними — это вопрос удобства, читаемости кода и эффективности.

* **Рекурсия:** Код часто более лаконичный и ближе к математическому определению. Однако может приводить к ошибке переполнения стека (`Stack Overflow`) и дополнительным расходам на вызовы функций.
* **Итерация:** Обычно работает быстрее и использует меньше памяти (не расходует стек вызовов). Код может быть более громоздким.

**Пример: вычисление факториала**

**Рекурсивная реализация (из билета 3):**
```python
def factorial_recursive(n):
  if n <= 1:
    return 1
  else:
    return n * factorial_recursive(n - 1)
```

**Итеративная реализация:**
```python
def factorial_iterative(n):
  result = 1
  for i in range(1, n + 1):
    result *= i
  return result
```

Обе функции дают абсолютно одинаковый результат, что доказывает их эквивалентность для данной задачи.

```python
print(f"Рекурсия: 5! = {factorial_recursive(5)}")   # -> 120
print(f"Итерация:  5! = {factorial_iterative(5)}")    # -> 120
```

<a name="bilet5">.</a> 
## Билет 5. 
Амортизационный анализ. Метод усреднения. Метод предоплаты. Метод потенциалов.

### 1. Амортизационный анализ

**Амортизационный анализ** — это метод оценки средней стоимости последовательности операций. Он помогает определить среднюю **амортизированную стоимость** одной операции, даже если некоторые операции в последовательности очень "дорогие", а большинство — "дешёвые".

В отличие от анализа **худшего случая**, который фокусируется на самой дорогой операции, амортизационный анализ даёт более реалистичную оценку производительности на длинной дистанции.

**Аналогия:** Покупка годового проездного на метро.
* **Худший случай:** Первая поездка стоит очень дорого (цена всего проездного).
* **Амортизационный анализ:** Если вы ездите каждый день, стоимость каждой поездки (амортизированная стоимость) оказывается очень низкой.

Этот анализ особенно полезен для структур данных, где дорогие операции (например, перераспределение памяти в динамическом массиве) случаются редко.

***

### 2. Метод усреднения (Aggregate Method)

Это самый простой метод. Мы находим **общую стоимость** $T(n)$ для последовательности из $n$ операций в худшем случае. Затем амортизированная стоимость одной операции вычисляется как $T(n) / n$.

**Пример: Добавление элемента в динамический массив (как `list` в Python).**
* Пусть у нас есть массив, который удваивает свой размер, когда место заканчивается.
* **Дешёвая операция:** Добавление элемента, когда есть свободное место. Фактическая стоимость = 1.
* **Дорогая операция:** Добавление элемента, когда места нет. Нужно выделить новый массив вдвое большего размера, скопировать туда все $k$ старых элементов и добавить новый. Фактическая стоимость = $k + 1$.

**Анализ:** За $n$ операций добавления общее количество копирований не превысит $n$. Таким образом, общая стоимость всех операций (обычных добавлений и копирований) будет $O(n)$.
* **Общая стоимость $T(n)$:** $O(n)$
* **Амортизированная стоимость:** $T(n) / n = O(n) / n = O(1)$.
Вывод: в среднем каждая операция `append` стоит константное время $O(1)$.

***

### 3. Метод предоплаты (Accounting Method / Banker's Method)

Этот метод вводит понятие **"кредита"** 💳. Мы назначаем каждой операции **амортизированную стоимость**, которая может быть выше её реальной стоимости.

* **"Дешёвые" операции** платят больше, чем тратят. Разница ("сдача") откладывается в виде кредита на счёт структуры данных.
* **"Дорогие" операции** тратят больше, чем платят. Они покрывают недостаток за счёт накопленного кредита.

**Главное правило:** Общий кредит на счёте никогда не должен быть отрицательным.

**Пример: Динамический массив.**
1.  Назначим амортизированную стоимость операции `append` равной **3 единицам**.
2.  **Случай 1: Место есть.**
    * **Фактическая стоимость:** 1 единица (добавить элемент).
    * **Платим:** 3 единицы.
    * **Результат:** 1 единица тратится, 2 единицы уходят в "кредит".
3.  **Случай 2: Места нет (массив из $k$ элементов заполнен).**
    * **Фактическая стоимость:** $k+1$ единиц (скопировать $k$ элементов и добавить 1 новый).
    * К этому моменту в массиве уже есть $k$ элементов. Как минимум $k/2$ из них были добавлены после последнего расширения, и каждый внёс по 2 "кредитных" единицы. Накопленный кредит = $(k/2) \cdot 2 = k$ единиц.
    * **Платим:** 3 единицы за текущую операцию + $k$ единиц из накопленного кредита. Итого $k+3$.
    * **Результат:** Этой суммы достаточно, чтобы покрыть фактическую стоимость $k+1$.

Амортизированная стоимость в 3 единицы является достаточной, следовательно, она равна $O(1)$.

***

### 4. Метод потенциалов (Potential Method)

Это самый гибкий метод, похожий на метод предоплаты, но "кредит" здесь представлен в виде **потенциальной функции $\Phi$**.

* Потенциальная функция $\Phi(D)$ сопоставляет состоянию структуры данных $D$ некоторое число (потенциал).
* В начале $\Phi(D_0) = 0$ и $\Phi(D_i) \geq 0$ для всех состояний.
* Амортизированная стоимость $a_i$ операции вычисляется по формуле:
    $$a_i = c_i + \Delta\Phi = c_i + (\Phi(D_i) - \Phi(D_{i-1}))$$
    где $c_i$ — реальная стоимость, а $\Delta\Phi$ — изменение потенциала после операции.

**Идея:**
* "Дешёвые" операции должны **увеличивать** потенциал ($\Delta\Phi > 0$).
* "Дорогие" операции должны **уменьшать** потенциал ($\Delta\Phi < 0$). Большое падение потенциала компенсирует высокую реальную стоимость $c_i$.

**Пример: Динамический массив.**
* Определим потенциал как: $\Phi(D) = 2 \cdot \text{size} - \text{capacity}$.
* **Случай 1: Место есть (без расширения).**
    * `size` увеличивается на 1, `capacity` не меняется.
    * Реальная стоимость $c_i = 1$.
    * Изменение потенциала $\Delta\Phi = (2(\text{size}+1) - \text{capacity}) - (2\cdot\text{size} - \text{capacity}) = 2$.
    * Амортизированная стоимость $a_i = c_i + \Delta\Phi = 1 + 2 = 3$.
* **Случай 2: Место закончилось (расширение с $k$ до $2k$ элементов).**
    * `size` становится $k+1$, `capacity` становится $2k$.
    * Реальная стоимость $c_i = k+1$ (копирование + вставка).
    * Изменение потенциала $\Delta\Phi = (2(k+1) - 2k) - (2k - k) = (2) - (k) = 2-k$.
    * Амортизированная стоимость $a_i = c_i + \Delta\Phi = (k+1) + (2-k) = 3$.

В обоих случаях амортизированная стоимость получилась равной 3, то есть $O(1)$.

<a name="bilet6">.</a> 
## Билет 6. 
Составные типы данных. Стек. Интерфейс стека, анализ сложности различных операций. Реализация минимума на стеке.

### Составные типы данных

**Составные (или структурированные) типы данных** — это типы данных, которые строятся на основе других, более простых типов. Они позволяют объединять несколько значений в единое целое.

Примеры включают:
* **Массивы и списки:** упорядоченные коллекции элементов.
* **Классы и объекты:** структуры, объединяющие данные (поля) и функции для работы с ними (методы).
* **Абстрактные типы данных:** стек, очередь, дерево, граф.

---

### Стек: Основные понятия

**Стек (Stack)** — это линейная структура данных, работающая по принципу **LIFO (Last-In, First-Out)**, то есть "последним пришёл — первым ушёл".

**Аналогия:** стопка тарелок 🍽️. Вы можете положить новую тарелку только наверх и взять тоже только верхнюю. Элемент, который добавили последним, будет извлечён первым.

В Python стек легко реализуется на основе списка (`list`), где операция `append()` добавляет элемент на вершину, а `pop()` удаляет элемент с вершины.

---

### Интерфейс стека и анализ сложности

Стандартный интерфейс стека включает следующие операции. Все они при реализации на основе динамического массива (Python `list`) имеют сложность **$O(1)$**.

| Операция | Описание | Сложность |
| :--- | :--- | :---: |
| **`push(item)`** | Добавить элемент на вершину стека. | $O(1)$* |
| **`pop()`** | Удалить и вернуть элемент с вершины. | $O(1)$ |
| **`peek()`** | Посмотреть элемент на вершине, не удаляя. | $O(1)$ |
| **`is_empty()`**| Проверить, пуст ли стек. | $O(1)$ |
| **`size()`** | Вернуть количество элементов. | $O(1)$ |

*\*Сложность `push` является амортизированной $O(1)$ из-за редких операций расширения массива.*

**Пример реализации на Python:**
```python
class Stack:
  def __init__(self):
    self.items = []

  def is_empty(self):
    return not self.items

  def push(self, item):
    self.items.append(item)

  def pop(self):
    if not self.is_empty():
      return self.items.pop()
    return "Error: Stack is empty"

  def peek(self):
    if not self.is_empty():
      return self.items[-1]
    return "Error: Stack is empty"

  def size(self):
    return len(self.items)
```

---

### Реализация минимума на стеке

Задача состоит в том, чтобы добавить в стек метод **`get_min()`**, который возвращает минимальный элемент в стеке за **$O(1)$**.

Просто хранить одну переменную `min_value` не получится: если мы удалим этот минимум из стека, мы не узнаем, какой минимум стал новым, не просмотрев все оставшиеся элементы.

**Решение:** использовать **два стека**:
1.  **`main_stack`**: обычный стек для хранения всех элементов.
2.  **`min_stack`**: вспомогательный стек, на вершине которого всегда находится текущий минимум.

**Логика работы:**
* **`push(item)`**:
    1.  Добавляем `item` в `main_stack`.
    2.  Сравниваем `item` с вершиной `min_stack`. Если `min_stack` пуст или `item` **меньше либо равен** его вершине, добавляем `item` также и в `min_stack`.
* **`pop()`**:
    1.  Извлекаем элемент `item` из `main_stack`.
    2.  Если `item` равен вершине `min_stack`, извлекаем элемент и из `min_stack`. Это значит, что мы удалили текущий минимум, и теперь минимум — это следующий элемент в `min_stack`.
* **`get_min()`**:
    1.  Просто возвращаем вершину `min_stack`.

**Пример реализации `MinStack`:**
```python
class MinStack:
  def __init__(self):
    self.main_stack = []
    self.min_stack = []

  def push(self, item):
    self.main_stack.append(item)
    if not self.min_stack or item <= self.min_stack[-1]:
      self.min_stack.append(item)

  def pop(self):
    if not self.main_stack:
      return "Error: Stack is empty"
    
    item = self.main_stack.pop()
    if item == self.min_stack[-1]:
      self.min_stack.pop()
    return item

  def get_min(self):
    if not self.min_stack:
      return "Error: Stack is empty"
    return self.min_stack[-1]

# Демонстрация работы
stack = MinStack()
stack.push(5)
stack.push(2)
stack.push(10)
stack.push(1)
print(f"Текущий минимум: {stack.get_min()}") # -> Текущий минимум: 1

stack.pop() # удалили 1
print(f"Текущий минимум после pop: {stack.get_min()}") # -> Текущий минимум: 2

stack.pop() # удалили 10
print(f"Текущий минимум после pop: {stack.get_min()}") # -> Текущий минимум: 2
```

<a name="bilet7">.</a> 
## Билет 7. 
Очередь. Интерфейс очереди, реализация очереди на двух стеках, анализ сложности операций. 

### Очередь: Основные понятия

**Очередь (Queue)** — это линейная структура данных, работающая по принципу **FIFO (First-In, First-Out)**, то есть "первым пришёл — первым ушёл".

**Аналогия:** обычная очередь в магазине 🧍‍♂️🧍‍♀️🧍. Первый, кто встал в очередь, первым и будет обслужен на кассе. Новые люди добавляются в конец очереди.

---

### Интерфейс очереди

Стандартный интерфейс очереди включает следующие операции. При реализации с помощью специализированных структур (как `collections.deque` в Python), все они имеют сложность $O(1)$.

| Операция | Описание |
| :--- | :--- |
| **`enqueue(item)`** | Добавить элемент в конец (хвост) очереди. |
| **`dequeue()`** | Удалить и вернуть элемент из начала (головы) очереди. |
| **`peek()`** | Посмотреть элемент в начале очереди, не удаляя его. |
| **`is_empty()`**| Проверить, пуста ли очередь. |
| **`size()`** | Вернуть количество элементов в очереди. |

---

### Реализация очереди на двух стеках

Можно эффективно реализовать очередь, используя два стека. Напомним, что стек работает по принципу LIFO ("последний вошёл — первый вышел").

**Идея:**
* Один стек (`in_stack`) используется для **добавления** элементов (операция `enqueue`).
* Второй стек (`out_stack`) используется для **извлечения** элементов (операция `dequeue`).

**Логика работы:**
1.  **`enqueue(item)`**: Всегда просто добавляем элемент в `in_stack`.
2.  **`dequeue()`**:
    * Если `out_stack` **не пуст**, значит, в нём уже лежат элементы в правильном (FIFO) порядке. Просто извлекаем верхний элемент.
    * Если `out_stack` **пуст**, мы должны "перелить" в него все элементы из `in_stack`. При этом они перевернутся, и самый старый элемент из `in_stack` окажется на вершине `out_stack`. После этого извлекаем элемент.

Эта "переливающая" операция выполняется только тогда, когда `out_stack` опустошается.

#### Анализ сложности

* **`enqueue`**: Всегда одна операция `push` в `in_stack`. **Сложность: $O(1)$**.
* **`dequeue`**:
    * **Худший случай**: Если `out_stack` пуст, а в `in_stack` находится `n` элементов, нам нужно выполнить `n` операций `pop` из `in_stack` и `n` операций `push` в `out_stack`. **Сложность: $O(n)$**.
    * **Лучший случай**: Если `out_stack` не пуст, нужна всего одна операция `pop`. **Сложность: $O(1)$**.
    * **Амортизированная сложность**: Каждый элемент за всё время своего существования в очереди ровно один раз добавляется в `in_stack`, один раз перекладывается в `out_stack` и один раз извлекается из него. Таким образом, средняя стоимость операции `dequeue` на длинной последовательности сводится к константе. **Амортизированная сложность: $O(1)$**.

#### Пример реализации

```python
class QueueWithStacks:
  def __init__(self):
    self.in_stack = []
    self.out_stack = []

  def enqueue(self, item):
    """Добавляет элемент в очередь. Сложность O(1)."""
    self.in_stack.append(item)

  def _transfer_if_needed(self):
    """
    Вспомогательный метод. Если out_stack пуст,
    перекладывает элементы из in_stack.
    """
    if not self.out_stack:
      while self.in_stack:
        self.out_stack.append(self.in_stack.pop())

  def dequeue(self):
    """Извлекает элемент из очереди. Амортизированная сложность O(1)."""
    if self.is_empty():
      return "Error: Queue is empty"
    self._transfer_if_needed()
    return self.out_stack.pop()

  def peek(self):
    """Смотрит на первый элемент. Амортизированная сложность O(1)."""
    if self.is_empty():
      return "Error: Queue is empty"
    self._transfer_if_needed()
    return self.out_stack[-1]

  def is_empty(self):
    return not self.in_stack and not self.out_stack

# Демонстрация работы
q = QueueWithStacks()
q.enqueue(1)
q.enqueue(2)
q.enqueue(3)

print(q.dequeue()) # -> 1. out_stack был пуст, элементы [1, 2, 3] переложились
print(q.peek())    # -> 2. out_stack не пуст, просто смотрим на вершину
print(q.dequeue()) # -> 2. out_stack не пуст, извлекаем
```


<a name="bilet8">.</a>
## Билет 8. 
Приоритетная очередь. Бинарная куча, основные понятия и интерфейс. Сортировка кучей. Поиск кратчайших путей во взвешенных графах.

### 1. Приоритетная очередь

**Приоритетная очередь (Priority Queue)** — это абстрактный тип данных, похожий на обычную очередь, но у каждого элемента есть **приоритет**. Элементы с более высоким приоритетом обрабатываются раньше, чем элементы с низким. Если приоритеты равны, порядок обработки обычно соответствует FIFO.

**Аналогия:** отделение неотложной помощи в больнице 🏥. Пациентов принимают на основе тяжести их состояния (приоритета), а не в порядке живой очереди.

---

### 2. Бинарная куча (Binary Heap)

**Бинарная куча** — это наиболее распространённый способ реализации приоритетной очереди. Это **полное бинарное дерево**, которое удовлетворяет **свойству кучи**.

* **Полное бинарное дерево:** все уровни дерева полностью заполнены, за исключением, возможно, последнего, который заполняется слева направо. Это позволяет эффективно хранить кучу в виде обычного массива.
* **Свойство кучи:**
    * **Min-heap (минимальная куча):** значение в любой вершине *меньше или равно* значениям в её дочерних вершинах. Самый маленький элемент всегда находится в корне.
    * **Max-heap (максимальная куча):** значение в любой вершине *больше или равно* значениям в её дочерних вершинах. Самый большой элемент всегда в корне.

#### Интерфейс и сложность

| Операция | Описание | Сложность |
| :--- | :--- | :---: |
| **`insert(item)`** | Добавить новый элемент, сохраняя свойство кучи. | $O(\log n)$ |
| **`extract_min()`** | Извлечь и вернуть корень (элемент с высшим приоритетом). | $O(\log n)$ |
| **`peek()`** | Посмотреть на корень, не извлекая его. | $O(1)$ |

В Python модуль `heapq` реализует min-heap.

```python
import heapq

# heapq работает со списком Python, превращая его в min-heap
priority_queue = []

heapq.heappush(priority_queue, (2, 'задача среднего приоритета'))
heapq.heappush(priority_queue, (1, 'задача высокого приоритета'))
heapq.heappush(priority_queue, (3, 'задача низкого приоритета'))

# Извлекаем элемент с наименьшим значением (высшим приоритетом)
highest_priority_task = heapq.heappop(priority_queue)
print(highest_priority_task) # -> (1, 'задача высокого приоритета')
```

---

### 3. Сортировка кучей (Heapsort)

**Сортировка кучей** — это эффективный алгоритм сортировки, использующий структуру бинарной кучи.

**Алгоритм (для сортировки по возрастанию):**
1.  **Построение кучи (Heapify):** Преобразуем исходный массив в **max-heap**. Самый большой элемент окажется в корне. Это можно сделать за $O(n)$.
2.  **Сортировка:**
    * Меняем местами корневой (максимальный) элемент с последним элементом в куче.
    * "Исключаем" этот последний элемент из рассмотрения — теперь он на своём отсортированном месте.
    * Восстанавливаем свойство max-heap для оставшейся части, "просеивая вниз" новый корень.
    * Повторяем процесс, пока в куче не останется один элемент.

**Сложность:**
* **Временная:** $O(n \log n)$ для худшего, среднего и лучшего случаев.
* **Пространственная:** $O(1)$, так как сортировка происходит на месте (in-place).

```python
def heapify(arr, n, i):
    """Превращает поддерево с корнем i в max-heap."""
    largest = i
    left = 2 * i + 1
    right = 2 * i + 2

    if left < n and arr[left] > arr[largest]:
        largest = left
    if right < n and arr[right] > arr[largest]:
        largest = right
    if largest != i:
        arr[i], arr[largest] = arr[largest], arr[i]
        heapify(arr, n, largest)

def heap_sort(arr):
    n = len(arr)
    # 1. Строим max-heap
    for i in range(n // 2 - 1, -1, -1):
        heapify(arr, n, i)
    
    # 2. Извлекаем элементы один за другим
    for i in range(n - 1, 0, -1):
        arr[i], arr[0] = arr[0], arr[i]  # Перемещаем корень в конец
        heapify(arr, i, 0) # Восстанавливаем кучу для оставшихся элементов
    return arr

my_array = [12, 11, 13, 5, 6, 7]
print(f"Отсортированный массив: {heap_sort(my_array)}")
# -> Отсортированный массив: [5, 6, 7, 11, 12, 13]
```

---

### 4. Поиск кратчайших путей во взвешенных графах

Приоритетные очереди играют ключевую роль в эффективных алгоритмах поиска кратчайших путей, в первую очередь в **алгоритме Дейкстры**.

**Алгоритм Дейкстры:** находит кратчайшие пути от одной стартовой вершины до всех остальных вершин во взвешенном графе **без рёбер с отрицательным весом**.

**Роль приоритетной очереди:**
1.  В приоритетную очередь помещаются вершины, которые предстоит посетить.
2.  **Приоритетом** каждой вершины является её текущее известное **кратчайшее расстояние** от старта.
3.  На каждом шаге алгоритм извлекает из очереди вершину с **наименьшим расстоянием** (т.е. с наивысшим приоритетом).
4.  Затем он просматривает её соседей и, если находит более короткий путь до них, обновляет их расстояние (приоритет) в очереди.

Использование бинарной кучи позволяет находить следующую ближайшую вершину за $O(\log V)$, где $V$ — число вершин, что делает весь алгоритм Дейкстры очень быстрым ($O(E \log V)$).

<a name="bilet9">.</a> 
## Билет 9. 
Основы теории графов. Математическое представление графов, списки смежности, матрицы смежности, матрицы инцидентности. Обход графа в ширину. 

## Основы теории графов

**Граф** $G=(V, E)$ — это математическая структура, состоящая из двух множеств:

  * **$V$ (Vertices)**: множество **вершин** (или узлов).
  * **$E$ (Edges)**: множество **рёбер**, которые соединяют пары вершин.

Графы бывают **направленными** (рёбра имеют направление, как улица с односторонним движением) и **ненаправленными** (рёбра симметричны). Также они могут быть **взвешенными**, когда каждому ребру присвоено число (вес), например, расстояние между городами.

-----

## Представление графов

Рассмотрим простой ненаправленный граф для примеров:

  * Вершины V = {0, 1, 2, 3}
  * Рёбра E = {(0, 1), (0, 2), (1, 2), (1, 3)}

\<img src="[https://i.imgur.com/yVb8YDE.png](https://i.imgur.com/yVb8YDE.png)" width="250"/\>

### 1\. Списки смежности (Adjacency Lists)

Это самый распространённый способ, особенно для разреженных графов (где мало рёбер). Для каждой вершины хранится список её соседей.

  * **Представление:** Словарь (или массив списков), где ключ — вершина, а значение — список смежных вершин.
  * **Плюсы:** Экономия памяти для разреженных графов. Быстрое получение всех соседей вершины.
  * **Минусы:** Проверка существования ребра `(u, v)` занимает время $O(k)$, где $k$ — число соседей вершины `u`.

<!-- end list -->

```python
# Для нашего примера графа
adjacency_list = {
    0: [1, 2],
    1: [0, 2, 3],
    2: [0, 1],
    3: [1]
}
```

### 2\. Матрица смежности (Adjacency Matrix)

Это матрица размером $|V| \\times |V|$. Элемент `M[i][j]` равен 1, если между вершинами `i` и `j` есть ребро, и 0 — если нет. Для взвешенных графов матрица хранит веса рёбер.

  * **Представление:** Двумерный массив.
  * **Плюсы:** Быстрая проверка наличия ребра `(i, j)` за $O(1)$.
  * **Минусы:** Требует $O(V^2)$ памяти, что расточительно для разреженных графов.

<!-- end list -->

```python
# Для нашего примера графа
#      0  1  2  3
adjacency_matrix = [
    [0, 1, 1, 0],  # 0
    [1, 0, 1, 1],  # 1
    [1, 1, 0, 0],  # 2
    [0, 1, 0, 0]   # 3
]
```

### 3\. Матрица инцидентности (Incidence Matrix)

Это матрица размером $|V| \\times |E|$. Элемент `M[i][j]` равен 1, если вершина `i` является концом ребра `j`.

  * **Представление:** Двумерный массив.
  * **Плюсы:** Удобна для некоторых теоретических задач и алгоритмов, связанных с потоками.
  * **Минусы:** Редко используется на практике для общих задач (обход, поиск путей), так как требует $O(V \\cdot E)$ памяти.

<!-- end list -->

```python
# Для нашего примера графа
# Рёбра: e0=(0,1), e1=(0,2), e2=(1,2), e3=(1,3)
#      e0 e1 e2 e3
incidence_matrix = [
    [1, 1, 0, 0], # 0
    [1, 0, 1, 1], # 1
    [0, 1, 1, 0], # 2
    [0, 0, 0, 1]  # 3
]
```

-----

## Обход графа в ширину (BFS, Breadth-First Search)

**Обход в ширину** — это алгоритм обхода графа, который исследует вершины "слой за слоем". Он начинает с исходной вершины, посещает всех её соседей, затем соседей этих соседей и так далее. Для своей работы BFS использует **очередь (Queue)** FIFO.

**Основное применение:** поиск кратчайшего пути в **невзвешенном** графе.

### Алгоритм BFS

1.  Создать **очередь** и добавить в неё стартовую вершину.
2.  Создать множество `visited` для отслеживания посещённых вершин и добавить в него стартовую вершину.
3.  Пока очередь не пуста:
    a. Извлечь вершину `u` из начала очереди.
    b. Обработать `u` (например, вывести на экран).
    c. Для каждого соседа `v` вершины `u`:
    i. Если `v` ещё не посещалась, добавить `v` в `visited` и в очередь.

### Пример на Python

```python
from collections import deque

def bfs(graph, start_node):
    """Обход графа в ширину."""
    visited = set()         # Множество для посещённых вершин
    queue = deque([start_node]) # Очередь для вершин к посещению
    visited.add(start_node)
    
    result = [] # Список для хранения порядка обхода
    
    while queue:
        vertex = queue.popleft() # Извлекаем вершину из начала очереди
        result.append(vertex)
        
        for neighbor in graph[vertex]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)
    return result

# Используем граф из примера выше
graph_example = {
    0: [1, 2],
    1: [0, 2, 3],
    2: [0, 1],
    3: [1]
}

# Начинаем обход с вершины 0
bfs_path = bfs(graph_example, 0)
print(f"Порядок обхода в ширину, начиная с 0: {bfs_path}")
# -> Порядок обхода в ширину, начиная с 0: [0, 1, 2, 3]
```

<a name="bilet10">.</a> 
## Билет 10. 
Основы теории графов. Математическое представление графов, списки смежности, матрицы смежности, матрицы инцидентности. Обход графа в глубину. Связность в ориентированных и неориентированных графах.

## Представление графов

**Граф** $G=(V, E)$ — это структура, состоящая из множества **вершин** ($V$) и множества **рёбер** ($E$), которые соединяют пары вершин.

Для примера используем простой ненаправленный граф:

  * Вершины V = {0, 1, 2, 3}
  * Рёбра E = {(0, 1), (0, 2), (1, 2), (1, 3)}

\<img src="[https://i.imgur.com/yVb8YDE.png](https://i.imgur.com/yVb8YDE.png)" width="250"/\>

Существует три основных способа его представить:

#### 1\. Списки смежности (Adjacency Lists)

Для каждой вершины хранится список её соседей. Это самый эффективный по памяти способ для графов, где рёбер мало.

```python
adjacency_list = {
    0: [1, 2],
    1: [0, 2, 3],
    2: [0, 1],
    3: [1]
}
```

#### 2\. Матрица смежности (Adjacency Matrix)

Матрица размером $|V| \\times |V|$, где `M[i][j] = 1`, если между вершинами `i` и `j` есть ребро. Позволяет проверить наличие ребра за $O(1)$, но требует $O(V^2)$ памяти.

```python
#      0  1  2  3
adjacency_matrix = [
    [0, 1, 1, 0],  # 0
    [1, 0, 1, 1],  # 1
    [1, 1, 0, 0],  # 2
    [0, 1, 0, 0]   # 3
]
```

#### 3\. Матрица инцидентности (Incidence Matrix)

Матрица размером $|V| \\times |E|$, где `M[i][j] = 1`, если вершина `i` является концом ребра `j`. Используется реже, в основном в теоретических задачах.

```python
# Рёбра: e0=(0,1), e1=(0,2), e2=(1,2), e3=(1,3)
#      e0 e1 e2 e3
incidence_matrix = [
    [1, 1, 0, 0], # 0
    [1, 0, 1, 1], # 1
    [0, 1, 1, 0], # 2
    [0, 0, 0, 1]  # 3
]
```

-----

## Обход графа в глубину (DFS, Depth-First Search)

**Обход в глубину** — это алгоритм, который исследует граф, уходя "вглубь" по одному пути настолько, насколько это возможно, прежде чем вернуться назад (backtracking). Для своей работы DFS использует **стек (Stack)** LIFO, который может быть неявным (системный стек вызовов при рекурсии) или явным.

**Основные применения:** поиск циклов, топологическая сортировка, поиск компонент связности.

### Алгоритм DFS (рекурсивный)

1.  Отметить текущую вершину `u` как посещённую.
2.  Обработать вершину `u`.
3.  Для каждого соседа `v` вершины `u`:
      * Если `v` ещё не посещалась, рекурсивно вызвать DFS для `v`.

### Пример на Python

```python
def dfs(graph, start_node, visited=None):
    """Рекурсивный обход графа в глубину."""
    if visited is None:
        visited = set()
    
    visited.add(start_node)
    print(start_node, end=' ') # Обрабатываем вершину
    
    for neighbor in graph[start_node]:
        if neighbor not in visited:
            dfs(graph, neighbor, visited)

# Используем граф из примера
graph_example = {
    0: [1, 2],
    1: [0, 2, 3],
    2: [0, 1],
    3: [1]
}

print("Порядок обхода в глубину, начиная с 0:", end=' ')
dfs(graph_example, 0)
# -> Порядок обхода в глубину, начиная с 0: 0 1 2 3 
# (Другой возможный порядок: 0 2 1 3)
```

-----

## Связность в графах

### Неориентированные графы

  * **Связный граф (Connected Graph):** Граф называется связным, если между любыми двумя его вершинами существует путь.
  * **Компоненты связности (Connected Components):** Если граф не является связным, он распадается на несколько **компонент связности**. Каждая такая компонента — это максимальный подграф, в котором все вершины связаны друг с другом. Найти все компоненты можно, запуская DFS или BFS из каждой ещё не посещённой вершины.

### Ориентированные графы (Digraphs)

Для ориентированных графов понятие связности сложнее.

  * **Слабая связность (Weakly Connected):** Граф является слабосвязным, если он становится связным после того, как все его ориентированные рёбра заменяются на неориентированные. То есть, можно добраться из любой вершины в любую, если игнорировать направления стрелок.
  * **Сильная связность (Strongly Connected):** Граф является сильносвязным, если для любой пары вершин `(u, v)` существует путь из `u` в `v` **и** путь из `v` в `u`.
  * **Компоненты сильной связности (Strongly Connected Components, SCC):** Это максимальные сильносвязные подграфы. Найти их можно с помощью специальных алгоритмов, таких как **алгоритм Тарьяна** или **алгоритм Косарайю**.

<a name="bilet11">.</a> 
## Билет 11. 
Поиск кратчайших путей во взвешенных графах. Алгоритм Дейкстры.

## Поиск кратчайших путей: Алгоритм Дейкстры

**Алгоритм Дейкстры (Dijkstra's Algorithm)** — это "жадный" алгоритм для нахождения кратчайших путей от одной стартовой вершины до всех остальных вершин во взвешенном графе.

**Важнейшее условие:** алгоритм корректно работает только для графов, где **все веса рёбер неотрицательные** (≥ 0).

---

### Основная идея

Алгоритм делит все вершины на два множества:
1.  Вершины, для которых кратчайший путь уже **точно найден**.
2.  Вершины, для которых кратчайший путь ещё **не найден** (или является предварительным).

На каждом шаге алгоритм выбирает "ближайшую" вершину из второго множества, для которой текущее расстояние минимально, и переносит её в первое. Затем он обновляет расстояния до её соседей. Этот процесс продолжается, пока не будут найдены кратчайшие пути до всех достижимых вершин.

Для эффективного поиска "ближайшей" вершины используется **приоритетная очередь**.

---

### Алгоритм

1.  **Инициализация:**
    * Создать словарь `distances` для хранения кратчайших расстояний. Установить расстояние до стартовой вершины равным 0, а до всех остальных — бесконечности ($\infty$).
    * Создать **приоритетную очередь** и добавить в неё стартовую вершину с приоритетом 0.

2.  **Основной цикл:**
    * Пока приоритетная очередь не пуста:
        a. Извлечь из очереди вершину `u` с наименьшим расстоянием (наивысшим приоритетом).
        b. Для каждого соседа `v` вершины `u` выполнить **релаксацию ребра**:
            * Вычислить новое расстояние до `v` через `u`: `new_distance = distances[u] + weight(u, v)`.
            * Если `new_distance` меньше, чем текущее значение `distances[v]`, то обновить `distances[v] = new_distance` и добавить `v` в приоритетную очередь с новым, более высоким приоритетом.

3.  **Завершение:**
    * Когда очередь опустеет, в словаре `distances` будут храниться кратчайшие пути от стартовой вершины до всех остальных.

### Сложность

При использовании приоритетной очереди на основе бинарной кучи, временная сложность алгоритма составляет **$O(E \log V)$**, где:
* $V$ — количество вершин.
* $E$ — количество рёбер.

---

### Пример на Python

```python
import heapq

def dijkstra(graph, start_node):
    """
    Находит кратчайшие пути от start_node до всех других вершин
    с помощью алгоритма Дейкстры.
    """
    # Словарь для хранения кратчайших расстояний
    distances = {node: float('inf') for node in graph}
    distances[start_node] = 0
    
    # Приоритетная очередь: (расстояние, вершина)
    priority_queue = [(0, start_node)]
    
    while priority_queue:
        # Извлекаем вершину с наименьшим расстоянием
        current_distance, current_node = heapq.heappop(priority_queue)
        
        # Если мы нашли более короткий путь ранее, пропускаем
        if current_distance > distances[current_node]:
            continue
            
        # Просматриваем соседей текущей вершины
        for neighbor, weight in graph[current_node].items():
            distance = current_distance + weight
            
            # Релаксация: если найден более короткий путь
            if distance < distances[neighbor]:
                distances[neighbor] = distance
                # Добавляем соседа в очередь с обновленным расстоянием
                heapq.heappush(priority_queue, (distance, neighbor))
                
    return distances

# Пример взвешенного графа в виде списков смежности
graph_example = {
    'A': {'B': 1, 'C': 4},
    'B': {'A': 1, 'C': 2, 'D': 5},
    'C': {'A': 4, 'B': 2, 'D': 1},
    'D': {'B': 5, 'C': 1}
}

# Находим кратчайшие пути от вершины 'A'
shortest_paths = dijkstra(graph_example, 'A')
print(f"Кратчайшие пути от 'A': {shortest_paths}")
# -> Кратчайшие пути от 'A': {'A': 0, 'B': 1, 'C': 3, 'D': 4}
```
